{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib.parse\n",
    "from pymongo import MongoClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, year, month, dayofmonth, from_unixtime, to_date\n",
    "from pyspark.ml.feature import StringIndexer, StandardScaler, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, LongType\n",
    "\n",
    "# Load MongoDB credentials\n",
    "def get_mongo_url():\n",
    "    login = {\n",
    "        \"host\": \"cluster1.tgo3l.mongodb.net\",\n",
    "        \"port\": 27017,\n",
    "        \"username\": \"guest_account\",\n",
    "        \"password\": \"e8RZETLbPWzhgvXH\"\n",
    "    }\n",
    "    username = login['username']\n",
    "    password = urllib.parse.quote(login['password'])\n",
    "    host = login['host']\n",
    "    url = f\"mongodb+srv://{username}:{password}@{host}/?retryWrites=true&w=majority\"\n",
    "    return url\n",
    "\n",
    "# Fetch data from MongoDB using pymongo\n",
    "def fetch_data_with_pymongo():\n",
    "    print(\"Fetching data from MongoDB...\")\n",
    "    mongo_url = get_mongo_url()\n",
    "    client = MongoClient(mongo_url)\n",
    "    db = client[\"steam\"]\n",
    "    collection = db[\"games\"]\n",
    "    \n",
    "    # Convert BSON types to Python-native types\n",
    "    def convert_bson_types(record):\n",
    "        record[\"_id\"] = str(record[\"_id\"])  # Convert ObjectId to string\n",
    "        if \"release_date\" in record:\n",
    "            record[\"release_date\"] = int(record[\"release_date\"])  # Convert bson.int64.Int64 to int\n",
    "        return record\n",
    "\n",
    "    # Prompt the user to choose 0 (all records) or 1 (5000 records)\n",
    "    choice = input(\"Enter 0 to fetch all records or 1 to fetch 5000 records: \")\n",
    "\n",
    "    # Fetch data based on the user's choice\n",
    "    if choice == \"0\":\n",
    "        print(\"Fetching all records from MongoDB.\")\n",
    "        data = [convert_bson_types(record) for record in collection.find()]\n",
    "    elif choice == \"1\":\n",
    "        print(\"Fetching 5000 records from MongoDB.\")\n",
    "        data = [convert_bson_types(record) for record in collection.find().limit(5000)]\n",
    "    else:\n",
    "        print(\"Invalid choice. Defaulting to 5000 records.\")\n",
    "        data = [convert_bson_types(record) for record in collection.find().limit(5000)]\n",
    "    client.close()\n",
    "    print(f\"Fetched {len(data)} records from MongoDB.\")\n",
    "    return data\n",
    "\n",
    "# Initialize SparkSession\n",
    "def create_spark_session():\n",
    "    print(\"Initializing Spark session...\")\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"MongoDBIntegration\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .getOrCreate()\n",
    "    print(\"Spark session initialized.\")\n",
    "    return spark\n",
    "\n",
    "# Define a schema for the data\n",
    "def get_schema():\n",
    "    return StructType([\n",
    "        StructField(\"_id\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"detailed_description\", StringType(), True),\n",
    "        StructField(\"price\", FloatType(), True),\n",
    "        StructField(\"dlc_count\", LongType(), True),  # Use LongType for large values\n",
    "        StructField(\"release_date\", LongType(), True),  # Unix timestamp in milliseconds\n",
    "        StructField(\"genres\", StringType(), True),\n",
    "        StructField(\"developers\", StringType(), True),\n",
    "        StructField(\"publishers\", StringType(), True),\n",
    "        StructField(\"peak_ccu\", LongType(), True)  # Use LongType if needed\n",
    "    ])\n",
    "\n",
    "# Convert pymongo data to PySpark DataFrame\n",
    "def convert_to_spark_dataframe(spark, data):\n",
    "    print(\"Converting pymongo data to PySpark DataFrame...\")\n",
    "    schema = get_schema()\n",
    "    df = spark.createDataFrame(data, schema=schema)\n",
    "    print(\"Data successfully converted to PySpark DataFrame.\")\n",
    "    return df\n",
    "\n",
    "# Transformation 1: Replace empty strings with null values and add 'dlc_check' flag\n",
    "def transform_data(df):\n",
    "    print(\"Starting data transformation...\")\n",
    "    df = df.withColumn(\n",
    "        \"name\", when(col(\"name\") == \"\", None).otherwise(col(\"name\"))\n",
    "    ).withColumn(\n",
    "        \"dlc_check\", when(col(\"dlc_count\") > 1, 1).otherwise(0)\n",
    "    ).withColumn(\n",
    "        \"release_date\", to_date(from_unixtime(col(\"release_date\") / 1000))  # Convert milliseconds to date\n",
    "    ).withColumn(\n",
    "        \"release_year\", year(\"release_date\")\n",
    "    ).withColumn(\n",
    "        \"release_month\", month(\"release_date\")\n",
    "    ).withColumn(\n",
    "        \"release_day\", dayofmonth(\"release_date\")\n",
    "    ).drop(\"release_date\")\n",
    "    df = df.dropna(subset=[\"name\", \"price\", \"peak_ccu\"])  # Remove rows with critical null values\n",
    "    print(\"Data transformation completed.\")\n",
    "    return df\n",
    "\n",
    "# Prepare data for modeling\n",
    "def prepare_data_for_modeling(df):\n",
    "    print(\"Preparing data for modeling...\")\n",
    "    categorical_columns = [\"genres\", \"developers\", \"publishers\"]\n",
    "    numerical_columns = [\"price\", \"dlc_count\"]\n",
    "    indexers = [StringIndexer(inputCol=column, outputCol=f\"{column}_index\") for column in categorical_columns]\n",
    "    assembler = VectorAssembler(inputCols=[f\"{column}_index\" for column in categorical_columns] + numerical_columns,\n",
    "                                 outputCol=\"features\")\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "    pipeline = Pipeline(stages=indexers + [assembler, scaler])\n",
    "    model = pipeline.fit(df)\n",
    "    df_preprocessed = model.transform(df)\n",
    "    print(\"Data preparation for modeling completed.\")\n",
    "    return df_preprocessed\n",
    "\n",
    "# Modified train_and_evaluate_models function with additional metrics\n",
    "def train_and_evaluate_models(df_preprocessed):\n",
    "    print(\"Training and evaluating models...\")\n",
    "    train, test = df_preprocessed.randomSplit([0.8, 0.2], seed=42)\n",
    "    models = {\n",
    "        \"Linear Regression\": LinearRegression(featuresCol=\"scaled_features\", labelCol=\"peak_ccu\"),\n",
    "        \"Decision Tree\": DecisionTreeRegressor(featuresCol=\"scaled_features\", labelCol=\"peak_ccu\"),\n",
    "        \"Random Forest\": RandomForestRegressor(featuresCol=\"scaled_features\", labelCol=\"peak_ccu\", numTrees=100)\n",
    "    }\n",
    "    results = {}\n",
    "    feature_importance = None\n",
    "    evaluator = RegressionEvaluator(labelCol=\"peak_ccu\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "    \n",
    "    # Add evaluator for additional metrics: RMSE, R2\n",
    "    rmse_evaluator = RegressionEvaluator(labelCol=\"peak_ccu\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    r2_evaluator = RegressionEvaluator(labelCol=\"peak_ccu\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name}...\")\n",
    "        fitted_model = model.fit(train)\n",
    "        predictions = fitted_model.transform(test)\n",
    "        \n",
    "        # Evaluate MAE, RMSE, and R2\n",
    "        mae = evaluator.evaluate(predictions)\n",
    "        rmse = rmse_evaluator.evaluate(predictions)\n",
    "        r2 = r2_evaluator.evaluate(predictions)\n",
    "        \n",
    "        results[model_name] = {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2}\n",
    "        \n",
    "        # Store feature importance for Random Forest\n",
    "        if model_name == \"Random Forest\":\n",
    "            feature_importance = fitted_model.featureImportances\n",
    "        \n",
    "        print(f\"{model_name} completed. MAE: {mae}, RMSE: {rmse}, R2: {r2}\")\n",
    "    \n",
    "    print(\"Model training and evaluation completed.\")\n",
    "    return results, feature_importance\n",
    "\n",
    "# Plot model comparison results\n",
    "def plot_results(results):\n",
    "    print(\"Plotting results...\")\n",
    "    # Extract metric values for plotting\n",
    "    mae_values = [metrics[\"MAE\"] for metrics in results.values()]\n",
    "    rmse_values = [metrics[\"RMSE\"] for metrics in results.values()]\n",
    "    r2_values = [metrics[\"R2\"] for metrics in results.values()]\n",
    "    \n",
    "    # Create a bar plot with different colors for each metric\n",
    "    x_pos = np.arange(len(results))\n",
    "    width = 0.25\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    ax.bar(x_pos - width, mae_values, width, label=\"MAE\", color='blue')\n",
    "    ax.bar(x_pos, rmse_values, width, label=\"RMSE\", color='green')\n",
    "    ax.bar(x_pos + width, r2_values, width, label=\"R2\", color='red')\n",
    "    \n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(results.keys())\n",
    "    ax.set_xlabel(\"Model\")\n",
    "    ax.set_ylabel(\"Metric Value\")\n",
    "    ax.set_title(\"Model Comparison Metrics\")\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"Results plotted.\")\n",
    "# New function to plot feature importance\n",
    "def plot_feature_importance(feature_importance, feature_names):\n",
    "    print(\"Plotting feature importance...\")\n",
    "    # Convert feature importance to numpy array\n",
    "    importance_array = feature_importance.toArray()\n",
    "    \n",
    "    # Sort features by importance\n",
    "    feature_importance_pairs = list(zip(feature_names, importance_array))\n",
    "    sorted_pairs = sorted(feature_importance_pairs, key=lambda x: x[1], reverse=True)\n",
    "    sorted_features = [x[0] for x in sorted_pairs]\n",
    "    sorted_importance = [x[1] for x in sorted_pairs]\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    y_pos = np.arange(len(sorted_features))\n",
    "    plt.barh(y_pos, sorted_importance, align='center')\n",
    "    plt.yticks(y_pos, sorted_features)\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('Random Forest Feature Importance')\n",
    "    \n",
    "    # Add value labels on the bars\n",
    "    for i, v in enumerate(sorted_importance):\n",
    "        plt.text(v, i, f'{v:.4f}', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"Feature importance plot completed.\")\n",
    "\n",
    "# Modified main function\n",
    "def main():\n",
    "    print(\"Starting main process...\")\n",
    "    data = fetch_data_with_pymongo()\n",
    "    spark = create_spark_session()\n",
    "    df = convert_to_spark_dataframe(spark, data)\n",
    "    df_transformed = transform_data(df)\n",
    "    df_preprocessed = prepare_data_for_modeling(df_transformed)\n",
    "    results, feature_importance = train_and_evaluate_models(df_preprocessed)\n",
    "    \n",
    "    # Plot model comparison\n",
    "    plot_results(results)\n",
    "    \n",
    "    # If feature importance exists, plot it\n",
    "    if feature_importance is not None:\n",
    "        feature_names = df_preprocessed.columns  # Adjust this if needed\n",
    "        plot_feature_importance(feature_importance, feature_names)\n",
    "    \n",
    "    print(\"Main process completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
